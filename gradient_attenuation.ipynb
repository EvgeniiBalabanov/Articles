{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda11119c0afe5647889a00de4d87fa958c",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проблема затухающего градиента в НС \n",
    "Рассмотрим задачу где есть:\n",
    "- ***НС*** с 4 слоями\n",
    "- 1 слой - 1 нейрон\n",
    "- Формула ***НС*** $ f = a_4(w_4*a_3(w_3*a_2(w_2*a_1(w_1*x))))$ \n",
    "\n",
    "     где:\n",
    "     * $f$ - выход из ***НС***\n",
    "     * $x$ - входной скаляр \n",
    "     * $w_n$ - веса $n$-ного слоя\n",
    "     * $a_n$ - нелинейная функция активации на $n$-ном слое\n",
    "- Требуется найти градиент по всем $w_n$\n",
    "\n",
    "     если:\n",
    "     * $a_n = Tanh()$ \n",
    "     * $a_n = ReLU()$ \n",
    "---\n",
    "\n",
    "## Разложим формулу и применим правило цепочки: \n",
    "\n",
    "$f_1 = tanh(w_1*x)$\n",
    "\n",
    "$f_2 = tanh(w_2*f_1)$\n",
    "\n",
    "$f_3 = tanh(w_3*f_2)$\n",
    "\n",
    "$f = tanh(w_4*f_3)$\n",
    "\n",
    "$\\frac{d(f)}{d(w_4)} = \\frac{d(f)}{d(f_3)} * f_3$\n",
    "\n",
    "$\\frac{d(f)}{d(w_3)} = \\frac{d(f)}{d(f_3)} * \\frac{d(f_3)}{d(f_2)} * f_2$\n",
    "\n",
    "$\\frac{d(f)}{d(w_2)} = \\frac{d(f)}{d(f_3)} * \\frac{d(f_3)}{d(f_2)} * \\frac{d(f_2)}{d(f_1)} * f_1$\n",
    "\n",
    "$\\frac{d(f)}{d(w_1)} = \\frac{d(f)}{d(f_3)} * \\frac{d(f_3)}{d(f_2)} * \\frac{d(f_2)}{d(f_1)} * \\frac{d(f_1)}{d(x)} * x$\n",
    "\n",
    "$tanh'(x) = 1 - tanh^2(x)$\n",
    "\n",
    "---\n",
    "## Решение\n",
    "\n",
    "\n",
    "### Для начала импортируем $tanh$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Инициализируем $x$ и $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 100\n",
    "w_1, w_2, w_3, w_4 = [1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_1 = tanh(w_1*x)\n",
    "f_2 = tanh(w_2*f_1)\n",
    "f_3 = tanh(w_3*f_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_w4 = (1-tanh(f_3)**2)*f_3\n",
    "grad_w3 = (1-tanh(f_3)**2)*(1-tanh(f_2)**2)*f_2\n",
    "grad_w2 = (1-tanh(f_3)**2)*(1-tanh(f_2)**2)*(1-tanh(f_1)**2)*f_1\n",
    "grad_w1 = (1-tanh(f_3)**2)*(1-tanh(f_2)**2)*(1-tanh(f_1)**2)*(1-tanh(x)**2)*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Создадим функцию для вывода информации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(fn_act,grads_w):\n",
    "    print(fn_act)\n",
    "    for n, grad in enumerate(grads_w):\n",
    "        print(f\"\\tГрадиент W на {n+1} слое:\\t{grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Выведем информацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Tanh:\n\tГрадиент W на 1 слое:\t0.0\n\tГрадиент W на 2 слое:\t0.16770685876939032\n\tГрадиент W на 3 слое:\t0.3041246830975466\n\tГрадиент W на 4 слое:\t0.436145382444544\n"
    }
   ],
   "source": [
    "show(\"Tanh:\",(grad_w1,grad_w2,grad_w3,grad_w4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Теперь посчитаем это с помощью PyTorch-autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Tanh,ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sclr = lambda v: torch.tensor(v,requires_grad=True)\n",
    "x = sclr(100.)\n",
    "w_1, w_2, w_3, w_4 =  [sclr(1.) for _ in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Tanh()\n\tГрадиент W на 1 слое:\t0.0\n\tГрадиент W на 2 слое:\t0.16770686209201813\n\tГрадиент W на 3 слое:\t0.30412471294403076\n\tГрадиент W на 4 слое:\t0.4361453950405121\nReLU()\n\tГрадиент W на 1 слое:\t100.0\n\tГрадиент W на 2 слое:\t100.0\n\tГрадиент W на 3 слое:\t100.0\n\tГрадиент W на 4 слое:\t100.0\n"
    }
   ],
   "source": [
    "for a in [Tanh(),ReLU()]:\n",
    "    f = lambda x: a(w_4 * a(w_3 * a(w_2 * a(w_1 * x))))\n",
    "    y = f(x)\n",
    "    y.backward()\n",
    "    show(a,(w_1.grad,w_2.grad,w_3.grad,w_4.grad))\n",
    "    for var in [w_1.grad,w_2.grad,w_3.grad,w_4.grad]:\n",
    "        var.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Вывод\n",
    " Из данных наблюдений следует, что функция активации $Tanh$, которая активно\n",
    " \n",
    " применялась в LeNet(1998), в некоторых ситуациях, сильно способствует затуханию градиента.\n",
    " \n",
    " Что в свою очередь, делает почти что, невозможным её примение в глубоких ***НС***."
   ]
  }
 ]
}